services:

  # ---
  # --- AI Interfaces & Orchestration ---
  # ---

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    ports:
      - "3001:3001"
    environment:
      - VECTOR_DB_TYPE=qdrant
      - VECTOR_DB_HOST=qdrant
      - VECTOR_DB_PORT=6333
      - VECTOR_DB_API_KEY=qdrant
      # <<< CHANGED â€” now points to WSL-native Ollama container
      - EXTERNAL_LLM_ENDPOINT=http://ollama:11434
      - EXTERNAL_LLM_MODEL=qwen3-coder:latest
      - STORAGE_DIR=/app/server/storage
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${BASE_DIR}:/anythingLLM/app/server/storage
      - ${BASE_DIR}/anythingLLM/hotdir:/app/collector/hotdir
      - ./anythingLLM/.env:/app/server/.env
    depends_on:
      - qdrant
      - ollama
    restart: unless-stopped

  # ---
  # --- Databases ---
  # ---

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ${BASE_DIR}/qdrant/storage:/qdrant/storage
    restart: unless-stopped

  mcp-bridge:
    build:
      context: .
      dockerfile: Dockerfile.qdrant-mcp
    container_name: mcp-bridge
    environment:
      - QDRANT_URL=http://qdrant:6333
      - FASTMCP_HOST=0.0.0.0
      - FASTMCP_PORT=8000
      - UVICORN_HOST=0.0.0.0
      - UVICORN_PORT=8000
      - FASTEMBED_CACHE_PATH=/app/fastembed_cache
    ports:
      - "8000:8000"
    volumes:
      - ${BASE_DIR}/qdrant/mcp:/app/fastembed_cache
    depends_on:
      - qdrant
      - ollama
    restart: unless-stopped

  # ---
  # --- NEW: WSL-native Ollama ---
  # ---

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_ORIGINS=*
      - OLLAMA_CONTEXT_LENGTH=12288
      - PYTHONUTF8=1
      - OLLAMA_KEEP_ALIVE=-1         # Keeps model in VRAM indefinitely
      - OLLAMA_FLASH_ATTENTION=1     # Significant speed boost for 12GB+ cards
    ports:
      - "11434:11434"
    volumes:
      - ${BASE_DIR}/ollama:/root/.ollama
    deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]      
    restart: unless-stopped
