services:

  # ---
  # --- AI Interfaces & Orchestration ---
  # ---

  openwebui:
    image: ghcr.io/open-webui/open-webui:cuda
    container_name: openwebui
    ports:
      - "3000:8080"
    volumes:
      - ${BASE_DIR}/openwebui/data:/app/backend/data
    environment:
      # <<< CHANGED — now points to WSL-native Ollama container
      - OLLAMA_BASE_URL=http://ollama:11434
      - COMFYUI_BASE_URL=http://comfyui:8188
      - WEBUI_AUTH=false
      - ENABLE_RAG_WEB_SEARCH=True
      - THREAD_POOL_SIZE=100
      - ENABLE_BASE_MODELS_CACHE=True
      - MODELS_CACHE_TTL=900 # Cache for 15 minutes
      - RAG_WEB_SEARCH_ENGINE=searxng
      - SEARXNG_QUERY_URL=http://searxng:8080/search?format=json&q=<query>
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - searxng
      - ollama
      - qdrant
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: always

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    ports:
      - "3001:3001"
    environment:
      - VECTOR_DB_TYPE=qdrant
      - VECTOR_DB_HOST=qdrant
      - VECTOR_DB_PORT=6333
      - VECTOR_DB_API_KEY=qdrant
      # <<< CHANGED — now points to WSL-native Ollama container
      - EXTERNAL_LLM_ENDPOINT=http://ollama:11434
      - EXTERNAL_LLM_MODEL=qwen3-coder:latest
      - STORAGE_DIR=/app/server/storage
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${BASE_DIR}:/anythingLLM/app/server/storage
      - ${BASE_DIR}/anythingLLM/hotdir:/app/collector/hotdir
      - ./anythingLLM/.env:/app/server/.env
    depends_on:
      - qdrant
      - ollama
    restart: unless-stopped

  # ---
  # --- Image Generation ---
  # custom_nodes/ComfyUI-Manager/config.ini
  # auto_update = false
  # check_registry = false
  # ---

  comfyui:
    build:
      context: .
      dockerfile: Dockerfile.comfyui
    container_name: comfyui
    ports:
      - "8188:8188"
    environment:
      - WEB_ENABLE_AUTH=false
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - YOLO_CONFIG_DIR=/app/user/default/ultralytics
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${BASE_DIR}/comfyui/models:/app/models
      - ${BASE_DIR}/comfyui/input:/app/input
      - ${BASE_DIR}/comfyui/output:/app/output
      - ${BASE_DIR}/comfyui/custom_nodes:/app/custom_nodes
    restart: always

  # ---
  # --- Search & Metadata ---
  # ---

  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    ports:
      - "8081:8080"
    volumes:
      - ./searxng:/etc/searxng
    environment:
      - SEARXNG_URL=http://design.tail201c18.ts.net:8081
      - SEARXNG_BASE_URL=http://design.tail201c18.ts.net:8081/
      - SEARXNG_REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    restart: unless-stopped

  # ---
  # --- Databases ---
  # ---

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ${BASE_DIR}/qdrant/storage:/qdrant/storage
    restart: unless-stopped

  mcp-bridge:
    build:
      context: .
      dockerfile: Dockerfile.qdrant-mcp
    container_name: mcp-bridge
    environment:
      - QDRANT_URL=http://qdrant:6333
      - FASTMCP_HOST=0.0.0.0
      - FASTMCP_PORT=8000
      - UVICORN_HOST=0.0.0.0
      - UVICORN_PORT=8000
      - FASTEMBED_CACHE_PATH=/app/fastembed_cache
    ports:
      - "8000:8000"
    volumes:
      - ${BASE_DIR}/qdrant/mcp:/app/fastembed_cache
    depends_on:
      - qdrant
      - ollama
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: redis
    command: redis-server --save 60 1 --loglevel warning
    restart: always

  # ---
  # --- NEW: WSL-native Ollama ---
  # ---

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_ORIGINS=*
      - OLLAMA_CONTEXT_LENGTH=12288
      - PYTHONUTF8=1
      - OLLAMA_KEEP_ALIVE=-1         # Keeps model in VRAM indefinitely
      - OLLAMA_FLASH_ATTENTION=1     # Significant speed boost for 12GB+ cards
    ports:
      - "11434:11434"
    volumes:
      - ${BASE_DIR}/ollama:/root/.ollama
    deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]      
    restart: unless-stopped

  # ---
  # --- NEW: OpenSearch (optional but integrated cleanly) ---
  # ---

  opensearch:
    image: opensearchproject/opensearch:2.17.0
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - DISABLE_SECURITY_PLUGIN=true
      - OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g
      - USER_AGENT=Mozilla/5.0 (compatible; OpenWebUI)
    ports:
      - "9200:9200"
    volumes:
      - ${BASE_DIR}/opensearch/data:/usr/share/opensearch/data
    restart: unless-stopped

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.17.0
    container_name: opensearch-dashboards
    environment:
      - OPENSEARCH_HOSTS=http://opensearch:9200
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    ports:
      - "5601:5601"
    depends_on:
      - opensearch
    restart: unless-stopped
